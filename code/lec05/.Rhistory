# Reshape to format needed by mxnet (width, height, channel, num)
dim(normed) <- c(224, 224, 3, 1)
return(normed)
}
normed <- preproc.image(im, mean.img)
prob <- predict(model, X=normed)
dim(prob)
max.idx <- max.col(t(prob))
max.idx
synsets <- readLines("Inception/synset.txt")
print(paste0("Predicted Top-class: ", synsets  [[max.idx]]))
url = 'http://writm.com/wp-content/uploads/2016/08/Cat-hd-wallpapers.jpg'
load.image(url)
im = load.image(url)
normed <- preproc.image(im, mean.img)
prob <- predict(model, X=normed)
prob <- predict(model, X=normed)
max.idx <- max.col(t(prob))
print(paste0("Predicted Top-class: ", synsets  [[max.idx]]))
url = 'https://images-na.ssl-images-amazon.com/images/G/01/img15/pet-products/small-tiles/23695_pets_vertical_store_dogs_small_tile_8._CB312176604_.jpg'
im = load.image(url)
normed <- preproc.image(im, mean.img)
prob <- predict(model, X=normed)
max.idx <- max.col(t(prob))
print(paste0("Predicted Top-class: ", synsets  [[max.idx]]))
pca.ae = h2o.deeplearning(x=predictors,
training_frame=trainX,
activation="Tanh",
autoencoder=T,
hidden=c(1024, 512, 256, 128, 2),
l1=1e-5,
ignore_const_cols=F,
epochs=10,
model_id="pca.ae"
)
h2o.saveModel(pca.ae, path="mnist")
pca.ae.train.features = h2o.deepfeatures(pca.ae, trainX, 5)
dim(pca.ae.train.features)
plot(as.matrix(pca.ae.train.features[1:1000,]),
pch=as.character( digit.data$train$y[1:1000] ),
col=c(1:10)[digit.data$train$y[1:1000]+1])
library(ElemStatLearn)
x <- mixture.example$x
g <- mixture.example$y
xnew <- mixture.example$xnew
px1 <- mixture.example$px1
px2 <- mixture.example$px2
gd <- expand.grid(x=px1, y=px2)
par(mar=rep(2,4))
plot(x, col=ifelse(g==1, "coral", "cornflowerblue"), axes=FALSE)
box()
df_train = data.frame(x1=x[,1], x2=x[,2], y=g)
df_test = data.frame(x1=xnew[,1], x2=xnew[,2])
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
df_train = data.frame(x=poly(x[,1], x[,2], degree=2, raw=TRUE), y=g)
df_test  = data.frame(x=poly(xnew[,1], xnew[,2], degree=2, raw=TRUE))
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- quadratic with interactions", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
df_train = data.frame(x=poly(x[,1], x[,2], degree=3, raw=TRUE), y=g)
df_test  = data.frame(x=poly(xnew[,1], xnew[,2], degree=3, raw=TRUE))
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- cubic with interactions", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
df_train = data.frame(x=poly(x[,1], x[,2], degree=7, raw=TRUE), y=g)
df_test  = data.frame(x=poly(xnew[,1], xnew[,2], degree=7, raw=TRUE))
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- 7th degree polynomial", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
lr_fit_7 = lr_fit
coef(lr_fit)
library(glmnet)
bigX_train = as.matrix(df_train[,-ncol(df_train)], nrow=nrow(df_train))
bigX_test = as.matrix(df_test, nrow=nrow(df_test))
lr_fit = cv.glmnet(x=bigX_train, y=g, family="binomial", nfolds=5)
prob = predict(lr_fit$glmnet.fit, bigX_test, type="response", s=lr_fit$lambda.min)
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- lasso", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
plot(lr_fit)
plot.glmnet(lr_fit, type.coef="coef")
lr_fit = cv.glmnet(x=bigX_train, y=g, family="binomial", nfolds=5, alpha=0)
prob = predict(lr_fit$glmnet.fit, bigX_test, type="response", s=lr_fit$lambda.min)
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- ridge penalty", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
library(textir)
library(glmnet)
data(we8there)
head(we8thereRatings)
we8thereCounts[1, we8thereCounts[1, ]!=0]            # words in the first review
we8thereCounts[6000, we8thereCounts[6000, ]!=0]      # words in the review 6000
we8thereCounts[6100, we8thereCounts[6100, ]!=0]      # words in the review 6000
dim(we8thereCounts)
y = ifelse(we8thereRatings$Overall>3, 1, 0)
y = as.factor(y)
set.seed(1)
glm_fit = cv.glmnet(x = we8thereCounts, y = y,
family = "binomial",
alpha = 1,                        # lasso - 1, ridge - 0
nfold = 5
)
plot(glm_fit)
plot(glm_fit$glmnet.fit, xvar = "lambda")
abline(v = log(glm_fit$lambda.1se), lty=2, col="red")
glm.coef = coef(glm_fit$glmnet.fit, s=glm_fit$lambda.1se)
o = order( glm.coef, decreasing = TRUE )
dimnames(glm.coef)[[1]][o[1:10]]
dimnames(glm.coef)[[1]][tail(o,10)]
glm.coef[tail(o,10)]
nnz(glm.coef)
sum(glm.coef!=0)
glm_fit_ridge = cv.glmnet(x = we8thereCounts, y = y,
family = "binomial",
alpha = 0,                        # lasso - 1, ridge - 0
nfold = 5
)
plot(glm_fit_ridge)
plot(glm_fit_ridge$glmnet.fit, xvar = "lambda")
abline(v = log(glm_fit_ridge$lambda.1se), lty=2, col="red")
o = order(glm.coef@x, decreasing = TRUE)
rownames(glm.coef)[glm.coef@i[o[1:10]]+1]
glm.coef@x[o[1:10]]
sum(glm.coef!=0)
glm.coef = coef(glm_fit_ridge$glmnet.fit, s=glm_fit_ridge$lambda.1se)
o = order(glm.coef@x, decreasing = TRUE)
rownames(glm.coef)[glm.coef@i[o[1:10]]+1]
glm.coef@x[o[1:10]]
sum(glm.coef!=0)
library(ElemStatLearn)
x <- mixture.example$x
g <- mixture.example$y
xnew <- mixture.example$xnew
px1 <- mixture.example$px1
px2 <- mixture.example$px2
gd <- expand.grid(x=px1, y=px2)
par(mar=rep(2,4))
plot(x, col=ifelse(g==1, "coral", "cornflowerblue"), axes=FALSE)
box()
df_train = data.frame(x1=x[,1], x2=x[,2], y=g)
df_test = data.frame(x1=xnew[,1], x2=xnew[,2])
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
df_train = data.frame(x=poly(x[,1], x[,2], degree=2, raw=TRUE), y=g)
df_test  = data.frame(x=poly(xnew[,1], xnew[,2], degree=2, raw=TRUE))
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- quadratic with interactions", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
df_train = data.frame(x=poly(x[,1], x[,2], degree=3, raw=TRUE), y=g)
df_test  = data.frame(x=poly(xnew[,1], xnew[,2], degree=3, raw=TRUE))
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- cubic with interactions", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
df_train = data.frame(x=poly(x[,1], x[,2], degree=7, raw=TRUE), y=g)
df_test  = data.frame(x=poly(xnew[,1], xnew[,2], degree=7, raw=TRUE))
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- 7th degree polynomial", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
coef(lr_fit_7)
lr_fit_7 = lr_fit
coef(lr_fit_7)
library(glmnet)
bigX_train = as.matrix(df_train[,-ncol(df_train)], nrow=nrow(df_train))
bigX_test = as.matrix(df_test, nrow=nrow(df_test))
lr_fit = cv.glmnet(x=bigX_train, y=g, family="binomial", nfolds=5)
prob = predict(lr_fit$glmnet.fit, bigX_test, type="response", s=lr_fit$lambda.min)
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- lasso", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
library(textir)
library(glmnet)
data(we8there)
head(we8thereRatings)
we8thereCounts[1, we8thereCounts[1, ]!=0]            # words in the first review
we8thereCounts[6000, we8thereCounts[6000, ]!=0]      # words in the review 6000
we8thereCounts[61000, we8thereCounts[6100, ]!=0]      # words in the review 6000
we8thereCounts[6100, we8thereCounts[6100, ]!=0]      # words in the review 6000
y = ifelse(we8thereRatings$Overall>3, 1, 0)
y = as.factor(y)
set.seed(1)
glm_fit = cv.glmnet(x = we8thereCounts, y = y,
family = "binomial",
alpha = 1,                        # lasso - 1, ridge - 0
nfold = 5
)
glm_fit = cv.glmnet(x = we8thereCounts, y = y,
family = "binomial",
alpha = 1,                        # lasso - 1, ridge - 0
nfold = 5
)
plot(glm_fit)
plot(glm_fit$glmnet.fit, xvar = "lambda")
abline(v = log(glm_fit$lambda.1se), lty=2, col="red")
glm.coef = coef(glm_fit$glmnet.fit, s=glm_fit$lambda.1se)
sum(glm.coef!=0)
o = order( glm.coef, decreasing = TRUE )
dimnames(glm.coef)[[1]][o[1:10]]
glm.coef[o[1:10]]
dimnames(glm.coef)[[1]][tail(o,10)]
glm.coef[tail(o,10)]
glm_fit_ridge = cv.glmnet(x = we8thereCounts, y = y,
family = "binomial",
alpha = 0,                        # lasso - 1, ridge - 0
nfold = 5
)
plot(glm_fit_ridge)
plot(glm_fit_ridge$glmnet.fit, xvar = "lambda")
abline(v = log(glm_fit_ridge$lambda.1se), lty=2, col="red")
library(ElemStatLearn)
x <- mixture.example$x
g <- mixture.example$y
xnew <- mixture.example$xnew
px1 <- mixture.example$px1
px2 <- mixture.example$px2
gd <- expand.grid(x=px1, y=px2)
par(mar=rep(2,4))
plot(x, col=ifelse(g==1, "coral", "cornflowerblue"), axes=FALSE)
box()
#dev.copy(pdf, "05_lr_mixture.pdf")
#dev.off()
## logistic regression
df_train = data.frame(x1=x[,1], x2=x[,2], y=g)
df_test = data.frame(x1=xnew[,1], x2=xnew[,2])
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
df_train = data.frame(x=poly(x[,1], x[,2], degree=2, raw=TRUE), y=g)
df_test  = data.frame(x=poly(xnew[,1], xnew[,2], degree=2, raw=TRUE))
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- quadratic with interactions", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
df_train = data.frame(x=poly(x[,1], x[,2], degree=3, raw=TRUE), y=g)
df_test  = data.frame(x=poly(xnew[,1], xnew[,2], degree=3, raw=TRUE))
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- cubic with interactions", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
df_train = data.frame(x=poly(x[,1], x[,2], degree=7, raw=TRUE), y=g)
df_test  = data.frame(x=poly(xnew[,1], xnew[,2], degree=7, raw=TRUE))
lr_fit = glm(y~., data=df_train, family=binomial())
prob = predict(lr_fit, df_test, type="response")
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- 7th degree polynomial", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
lr_fit_7 = lr_fit
coef(lr_fit_7)
library(glmnet)
lr_fit = cv.glmnet(x=bigX_train, y=g, family="binomial", nfolds=5)
prob = predict(lr_fit$glmnet.fit, bigX_test, type="response", s=lr_fit$lambda.min)
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- lasso", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
bigX_train = as.matrix(df_train[,-ncol(df_train)], nrow=nrow(df_train))
bigX_test = as.matrix(df_test, nrow=nrow(df_test))
lr_fit = cv.glmnet(x=bigX_train, y=g, family="binomial", nfolds=5)
prob = predict(lr_fit$glmnet.fit, bigX_test, type="response", s=lr_fit$lambda.min)
prob_lr <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob_lr, levels=0.5, labels="", xlab="", ylab="", main=
"logistic regression -- lasso", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
points(gd, pch=".", cex=1.2, col=ifelse(prob_lr>0.5, "coral", "cornflowerblue"))
box()
lr_fit_lasso = lr_fit
coef(lr_fit$glmnet.fit, s=lr_fit$lambda.min)
library(textir)
library(glmnet)
data(we8there)
head(we8thereRatings)
we8thereCounts[1, we8thereCounts[1, ]!=0]            # words in the first review
we8thereCounts[6000, we8thereCounts[6000, ]!=0]      # words in the review 6000
we8thereCounts[6100, we8thereCounts[6100, ]!=0]      # words in the review 6000
y = ifelse(we8thereRatings$Overall>3, 1, 0)
y = as.factor(y)
set.seed(1)
glm_fit = cv.glmnet(x = we8thereCounts, y = y,
family = "binomial",
alpha = 1,                        # lasso - 1, ridge - 0
nfold = 5
)
plot(glm_fit)
plot(glm_fit$glmnet.fit, xvar = "lambda")
abline(v = log(glm_fit$lambda.1se), lty=2, col="red")
glm.coef = coef(glm_fit$glmnet.fit, s=glm_fit$lambda.1se)
o = order( glm.coef, decreasing = TRUE )
dimnames(glm.coef)[[1]][o[1:10]]
glm.coef[o[1:10]]
# negative coefficients
dimnames(glm.coef)[[1]][tail(o,10)]
glm.coef[tail(o,10)]
glm_fit_ridge = cv.glmnet(x = we8thereCounts, y = y,
family = "binomial",
alpha = 0,                        # lasso - 1, ridge - 0
nfold = 5
)
plot(glm_fit_ridge)
plot(glm_fit_ridge$glmnet.fit, xvar = "lambda")
abline(v = log(glm_fit_ridge$lambda.1se), lty=2, col="red")
inClass = TRUE
source("mnist.helper.R")
MNIST_DIR = "/home/mkolar/projects/mlRepos/MLClassData/MNISTDigits"
digit.data = load_mnist(MNIST_DIR)
library(h2o)
h2oServer <- h2o.init(ip="localhost", port=54321, max_mem_size="4g", nthreads=2)
train_hex = as.h2o(data.frame(x=digit.data$train$x, y=digit.data$train$y))
test_hex = as.h2o(data.frame(x=digit.data$test$x, y=digit.data$test$y))
predictors = 1:784
response = 785
train_hex[,response] <- as.factor(train_hex[,response])
test_hex[,response] <- as.factor(test_hex[,response])
train_labels = train_hex[,response]
test_labels = test_hex[,response]
trainX = train_hex[,-response]
testX = test_hex[,-response]
test_rec_error = as.data.frame(h2o.anomaly(auto_encoder, testX))
nfeatures = 50
# train autoencoder on train_unsupervised
auto_encoder = h2o.deeplearning(x=predictors,
training_frame=trainX,
activation="Tanh",
autoencoder=T,
hidden=c(nfeatures),
l1=1e-5,
ignore_const_cols=F,
epochs=1)
train.ae.features = h2o.deepfeatures(auto_encoder, trainX, 1)
test.ae.features = h2o.deepfeatures(auto_encoder, testX, 1)
test_rec_error = as.data.frame(h2o.anomaly(auto_encoder, testX))
test_recon = predict(auto_encoder, testX)
plotDigits(test_recon, test_rec_error, c(1:25))
plotDigits(testX,   test_rec_error, c(4988:5012))
plotDigits(test_recon, test_rec_error, c(9976:10000))
plotDigits(testX,   test_rec_error, c(9976:10000))
plotDigits(testX,   test_rec_error, c(1:25))
inClass = TRUE
source("mnist.helper.R")
MNIST_DIR = "/home/mkolar/projects/mlRepos/MLClassData/MNISTDigits"
digit.data = load_mnist(MNIST_DIR)
library(h2o)
h2oServer <- h2o.init(ip="localhost", port=54321, max_mem_size="4g", nthreads=2)
h2oServer <- h2o.init(ip="localhost", port=54321, max_mem_size="4g", nthreads=2)
# we need to load data into h2o format
train_hex = as.h2o(data.frame(x=digit.data$train$x, y=digit.data$train$y))
test_hex = as.h2o(data.frame(x=digit.data$test$x, y=digit.data$test$y))
predictors = 1:784
response = 785
train_hex[,response] <- as.factor(train_hex[,response])
test_hex[,response] <- as.factor(test_hex[,response])
train_labels = train_hex[,response]
test_labels = test_hex[,response]
# create frames with input features only
# we will need these later for unsupervised training
trainX = train_hex[,-response]
testX = test_hex[,-response]
nfeatures = 50
# train autoencoder on train_unsupervised
auto_encoder = h2o.deeplearning(x=predictors,
training_frame=trainX,
activation="Tanh",
autoencoder=T,
hidden=c(nfeatures),
l1=1e-5,
ignore_const_cols=F,
epochs=1)
train.ae.features = h2o.deepfeatures(auto_encoder, trainX, 1)
test.ae.features = h2o.deepfeatures(auto_encoder, testX, 1)
# (passing it through the autoencoder model and computing mean square error (MSE) for each row)
test_rec_error = as.data.frame(h2o.anomaly(auto_encoder, testX))
# 3) VISUALIZE OUTLIERS
# Let's look at the test set points with low/median/high reconstruction errors.
# We will now visualize the original test set points and their reconstructions obtained
# by propagating them through the narrow neural net.
# Convert the test data into its autoencoded representation (pass through narrow neural net)
test_recon = predict(auto_encoder, testX)
plotDigits(test_recon, test_rec_error, c(1:25))
plotDigits(testX,   test_rec_error, c(1:25))
plotDigits(test_recon, test_rec_error, c(4988:5012))
plotDigits(testX,   test_rec_error, c(4988:5012))
plotDigits(test_recon, test_rec_error, c(9976:10000))
plotDigits(testX,   test_rec_error, c(9976:10000))
inClass = TRUE
source("mnist.helper.R")
MNIST_DIR = "/home/mkolar/projects/mlRepos/MLClassData/MNISTDigits"
digit.data = load_mnist(MNIST_DIR)
library(h2o)
h2oServer <- h2o.init(ip="localhost", port=54321, max_mem_size="4g", nthreads=2)
train_hex = as.h2o(data.frame(x=digit.data$train$x, y=digit.data$train$y))
test_hex = as.h2o(data.frame(x=digit.data$test$x, y=digit.data$test$y))
predictors = 1:784
response = 785
train_hex[,response] <- as.factor(train_hex[,response])
test_hex[,response] <- as.factor(test_hex[,response])
train_labels = train_hex[,response]
test_labels = test_hex[,response]
# create frames with input features only
# we will need these later for unsupervised training
trainX = train_hex[,-response]
testX = test_hex[,-response]
nfeatures = 50
# train autoencoder on train_unsupervised
auto_encoder = h2o.deeplearning(x=predictors,
training_frame=trainX,
activation="Tanh",
autoencoder=T,
hidden=c(nfeatures),
l1=1e-5,
ignore_const_cols=F,
epochs=1)
train.ae.features = h2o.deepfeatures(auto_encoder, trainX, 1)
test.ae.features = h2o.deepfeatures(auto_encoder, testX, 1)
# (passing it through the autoencoder model and computing mean square error (MSE) for each row)
test_rec_error = as.data.frame(h2o.anomaly(auto_encoder, testX))
# 3) VISUALIZE OUTLIERS
# Let's look at the test set points with low/median/high reconstruction errors.
# We will now visualize the original test set points and their reconstructions obtained
# by propagating them through the narrow neural net.
# Convert the test data into its autoencoded representation (pass through narrow neural net)
test_recon = predict(auto_encoder, testX)
plotDigits(test_recon, test_rec_error, c(1:25))
plotDigits(testX,   test_rec_error, c(1:25))
plotDigits(test_recon, test_rec_error, c(4988:5012))
plotDigits(testX,   test_rec_error, c(4988:5012))
plotDigits(test_recon, test_rec_error, c(9976:10000))
plotDigits(testX,   test_rec_error, c(9976:10000))
